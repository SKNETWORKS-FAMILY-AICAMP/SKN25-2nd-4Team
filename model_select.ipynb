{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5aefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "class CustomWinsorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, limits=[0.05, 0.05]):\n",
    "        self.limits = limits\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        from scipy.stats.mstats import winsorize\n",
    "        X_copy = X.copy()\n",
    "        if isinstance(X_copy, pd.DataFrame):\n",
    "            for col in X_copy.columns:\n",
    "                X_copy[col] = winsorize(X_copy[col], limits=self.limits)\n",
    "        else:\n",
    "            X_copy = winsorize(X_copy, limits=self.limits)\n",
    "        return X_copy\n",
    "\n",
    "num_cols = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'Point Earned']\n",
    "cat_cols = ['Geography', 'Gender', 'Card Type']\n",
    "pass_cols = ['HasBalance', 'Age_Group', 'IsActiveMember', 'NumOfProducts', \n",
    "             'Prod_is_1', 'Prod_is_2', 'Prod_ge_3', 'ZeroBal_Prod2', 'ZeroBal_Prod1', \n",
    "             'Prod2_Inactive', 'Inactive_Old',]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('winsorizer', CustomWinsorizer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "    ('pass', 'passthrough', pass_cols)\n",
    "])\n",
    "# --- [1] 파생변수 생성 함수 다시 정의 ---\n",
    "def add_custom_features(df):\n",
    "    X = df.copy()\n",
    "    X['HasBalance'] = (X['Balance'] > 0).astype(int)\n",
    "    X['BalanceSalaryRatio'] = X['Balance'] / (X['EstimatedSalary'] + 1e-6)\n",
    "    X['Age_Group'] = pd.cut(X['Age'], bins=[0, 30, 45, 60, 100], labels=[0, 1, 2, 3]).astype(int)\n",
    "    X['Prod_is_1'] = (X['NumOfProducts'] == 1).astype(int)\n",
    "    X['Prod_is_2'] = (X['NumOfProducts'] == 2).astype(int)\n",
    "    X['Prod_ge_3'] = (X['NumOfProducts'] >= 3).astype(int)\n",
    "    X['ZeroBal_Prod2'] = ((X['Balance'] == 0) & (X['NumOfProducts'] == 2)).astype(int)\n",
    "    X['ZeroBal_Prod1'] = ((X['Balance'] == 0) & (X['NumOfProducts'] == 1)).astype(int)\n",
    "    X['Prod2_Inactive'] = ((X['NumOfProducts'] == 2) & (X['IsActiveMember'] == 0)).astype(int)\n",
    "    X['Inactive_Old'] = ((X['IsActiveMember'] == 0) & (X['Age'] >= 45)).astype(int)\n",
    "    return X\n",
    "\n",
    "# --- [2] 데이터 로드 및 1차 분할 (8:2) ---\n",
    "df = pd.read_csv('Customer-Churn-Records.csv')\n",
    "X = df.drop(['RowNumber', 'CustomerId', 'Surname', 'Exited'], axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- [3] 파생변수 적용 및 전처리 ---\n",
    "# 80% 데이터 전체에 파생변수 생성\n",
    "X_train_full_enriched = add_custom_features(X_train_full)\n",
    "X_test_enriched = add_custom_features(X_test)\n",
    "\n",
    "# 파이프라인 적용\n",
    "X_train_final = preprocessor.fit_transform(X_train_full_enriched)\n",
    "X_test_final = preprocessor.transform(X_test_enriched)\n",
    "\n",
    "print(f\"✅ 파생변수 포함 완료!\")\n",
    "print(f\"최종 학습 데이터 크기: {X_train_final.shape}\") \n",
    "\n",
    "# --- [4] K-Fold 모델 학습 및 시각화 ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, recall_score, roc_auc_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 1. 모델 정의 (불균형 데이터 처리를 위한 가중치 옵션 추가)\n",
    "# 이전 단일 모델에서 효과를 보셨던 scale_pos_weight=4를 기준으로 설정합니다.\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        class_weight='balanced', \n",
    "        C=0.1,             # 규제 강도 (작을수록 강한 규제, 과적합 방지)\n",
    "        solver='liblinear', # 작은 데이터셋이나 이진 분류에 안정적\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,    # 트리를 더 많이 쌓아 안정성 확보\n",
    "        class_weight='balanced', \n",
    "        max_depth=10,        # 너무 깊은 트리는 과적합을 유발하므로 제한\n",
    "        min_samples_leaf=5,  # 한 잎 노드에 최소 샘플 수 (일반화 성능 향상)\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "   'XGBoost': XGBClassifier(\n",
    "    n_estimators=500,        # 트리 개수를 늘림 (정교함 상승)\n",
    "    learning_rate=0.01,      # 아주 낮게 설정하여 천천히 정밀하게 학습\n",
    "    max_depth=6,             # 트리 깊이를 6으로 제한 (과적합 방지)\n",
    "    min_child_weight=5,      # 관측치 수가 적은 노드 분할 억제 (일반화 성능)\n",
    "    gamma=0.2,               # 분할을 위한 최소 손실 감소 값 (트리 복잡도 제어)\n",
    "    subsample=0.8,           # 행 데이터의 80%만 무작위 사용 (폴드별 안정성)\n",
    "    colsample_bytree=0.8,     # 열 변수의 80%만 무작위 사용 (특정 변수 편향 방지)\n",
    "    scale_pos_weight=4,      # 이탈자(1) 가중치 (Recall 방어)\n",
    "    reg_alpha=0.1,           # L1 규제 (불필요한 변수 가중치 제거)\n",
    "    reg_lambda=1.0,          # L2 규제 (가중치 크기 억제)\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    "),\n",
    "    \n",
    "'LightGBM': LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=31,           # max_depth가 6~7일 때 가장 적절한 복잡도\n",
    "    max_depth=6,             # 과적합 방지를 위해 트리 깊이 제한\n",
    "    min_data_in_leaf=20,     # 한 잎에 포함될 최소 데이터 수 (안정성 강화)\n",
    "    feature_fraction=0.8,    # 트리 구축 시 변수 샘플링 비율 (XGB의 colsample과 유사)\n",
    "    bagging_fraction=0.8,     # 데이터 샘플링 비율\n",
    "    bagging_freq=5,          # 5번의 반복마다 배깅 수행\n",
    "    scale_pos_weight=4,\n",
    "    lambda_l1=0.1,           # L1 규제\n",
    "    lambda_l2=0.1,           # L2 규제\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "}\n",
    "\n",
    "\n",
    "# 2. K-Fold 및 설정\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "custom_threshold = 0.4  # Recall 확보를 위해 임계값을 0.4로 하향 조정 (기본값 0.5)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "results = []\n",
    "\n",
    "print(f\"데이터 크기 확인: X={X_train_final.shape[0]}, y={len(y_train_full)}\")\n",
    "print(f\"적용 임계값(Threshold): {custom_threshold}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 3. 모델별 학습 및 K-Fold 평가\n",
    "for name, model in models.items():\n",
    "    oof_probs = np.zeros(len(y_train_full))\n",
    "    fold_aucs, fold_f1s, fold_recalls, fold_precisions = [], [], [], []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train_final, y_train_full):\n",
    "        X_kf_train, X_kf_val = X_train_final[train_idx], X_train_final[val_idx]\n",
    "        y_kf_train, y_kf_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_kf_train, y_kf_train)\n",
    "        \n",
    "        # 확률값 계산\n",
    "        probs = model.predict_proba(X_kf_val)[:, 1]\n",
    "        \n",
    "        # [핵심] 임계값 적용: 0.5가 아닌 0.4를 기준으로 클래스 결정\n",
    "        preds = (probs >= custom_threshold).astype(int)\n",
    "        \n",
    "        fold_aucs.append(roc_auc_score(y_kf_val, probs))\n",
    "        fold_f1s.append(f1_score(y_kf_val, preds))\n",
    "        fold_recalls.append(recall_score(y_kf_val, preds))\n",
    "        fold_precisions.append(precision_score(y_kf_val, preds))\n",
    "        \n",
    "        oof_probs[val_idx] = probs\n",
    "\n",
    "    # 결과 평균 계산\n",
    "    avg_auc = np.mean(fold_aucs)\n",
    "    avg_f1 = np.mean(fold_f1s)\n",
    "    avg_recall = np.mean(fold_recalls)\n",
    "    avg_precision = np.mean(fold_precisions)\n",
    "    results.append([name, avg_auc, avg_f1, avg_recall, avg_precision])\n",
    "\n",
    "    # ROC 커브 시각화\n",
    "    fpr, tpr, _ = roc_curve(y_train_full, oof_probs)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {avg_auc:.3f})')\n",
    "    print(f\"✅ {name} 학습 및 Recall 최적화 완료\")\n",
    "\n",
    "# 4. 성능 지표 테이블 출력\n",
    "df_res = pd.DataFrame(results, columns=['Model', 'AUC', 'F1-Score', 'Recall', 'Precision'])\n",
    "print(\"\\n[5-Fold Cross Validation - Recall Optimized]\")\n",
    "print(df_res.to_string(index=False))\n",
    "\n",
    "# 5. ROC 커브 시각화 마무리\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title(f'ROC Curve Comparison (Threshold={custom_threshold})')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
